---
layout: post
title: Introducing the Capstone Project
---
# Categorizing News Articles Using NLP

## Backstory
For this analysis, I will take a deep dive into the world of Natural Language Processing in order to classify news articles based on the type of language they use. Given the rise of so-called "fake news" and "alternative facts", the Facebook algorithms that preferentially show a user  content that aligns with their viewing preferences, and resulting the "echo chamber" effect on users' newsfeeds, I've been thinking a lot about the intersection of machine learning, text classification, and journalism.

Initially, I was curious to see if there were readily detectable linguistic differences in news articles from reputable sources and those from the onslaught of .co look-alike sites that have been created with the goal of deceiving readers. However, I found myself entering a gray area - what to do about sites like Breitbart.com? Although I personally disagree with essentially any piece published there, the site falls into a strange middle zone between opinion and actual news. Some of the facts may be true, but they're presented through a very extreme lens. Additionally, many of the knock-off news sites offer a relatively sparse article list - they just don't publish a large enough corpus.

## Problem Statement

My goal for this project is to classify news articles by subject matter. At the most basic level, I hope to build a classifier that can differentiate between factual reporting and opinion pieces when trained on a corpus of articles published by the New York Times. If this is successful, I hope to increase predictive power in order to differentiate between business, world news, technology, science, sports, or arts articles.

## Progress So Far

The first step of this project is to acquire and store the articles I'll use to build my corpus. I've web-scraped article titles and text, as well as meta-data such as author, date of publication, and section (business, science, etc). Since I'm restricting this task to actual article content, I've ignored comments sections, clicks and shares, and publication metadata. These data are stored in a local postgres database before processing and analysis in Pandas.


![png](../../../images/database_screenshot.png)


## Next Steps

My next steps are to automate the scraping process, perhaps using the Mechanize library, and to familiarize myself with the NLTK library. Stay tuned for some exploratory data analysis!
